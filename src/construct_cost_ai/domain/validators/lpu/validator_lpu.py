"""
M√≥dulo de Valida√ß√£o LPU - Verifica discrep√¢ncias entre or√ßamento e base de pre√ßos.

Este m√≥dulo realiza a concilia√ß√£o entre o or√ßamento enviado pela construtora
e a base de dados oficial da LPU (Lista de Pre√ßos Unit√°rios), identificando discrep√¢ncias
nos valores com toler√¢ncia configur√°vel.
"""

__author__ = "Emerson V. Rafael (emervin)"
__copyright__ = "Copyright 2025, Construct Cost AI"
__credits__ = ["Emerson V. Rafael"]
__license__ = "MIT"
__version__ = "1.0.0"
__maintainer__ = "Emerson V. Rafael"
__email__ = "emersonssmile@gmail.com"
__status__ = "Development"

import sys
from pathlib import Path
from typing import Union, Tuple, List, Optional, Literal, NamedTuple

import pandas as pd

# Adiciona o diret√≥rio src ao path
base_dir = Path(__file__).parents[5]
sys.path.insert(0, str(Path(base_dir, "src")))

from config.config_logger import logger
from config.config_dynaconf import get_settings
from utils.data.data_functions import (
    read_data,
    export_data,
    cast_columns,
    transform_case,
    merge_data_with_columns,
)
from utils.lpu.lpu_functions import generate_region_group_combinations, split_regiao_grupo

settings = get_settings()


class ValidatorLPUError(Exception):
    """Exce√ß√£o base para erros do validador LPU."""

    pass


class FileNotFoundError(ValidatorLPUError):
    """Exce√ß√£o para arquivo n√£o encontrado."""

    pass


class MissingColumnsError(ValidatorLPUError):
    """Exce√ß√£o para colunas obrigat√≥rias ausentes."""

    pass


def load_budget(file_path: Union[str, Path]) -> pd.DataFrame:
    """
    Carrega o arquivo de or√ßamento.

    Args:
        path: Caminho para o arquivo de or√ßamento (Excel ou CSV)

    Returns:
        DataFrame com o or√ßamento carregado

    Raises:
        FileNotFoundError: Se o arquivo n√£o for encontrado
        MissingColumnsError: Se colunas obrigat√≥rias estiverem ausentes
    """
    file_path = Path(file_path)

    if not file_path.exists():
        raise FileNotFoundError(f"Arquivo de or√ßamento n√£o encontrado: {file_path}")

    # Colunas obrigat√≥rias
    required_columns = settings.get(
        "module_validator_lpu.budget_data.required_columns_with_types", []
    )

    # Coluna valor total
    column_total_value = settings.get("module_validator_lpu.column_total_value", "VALOR TOTAL")

    try:
        # Ler os dados de Or√ßamento e realizar pr√© processing
        df = transform_case(
            read_data(
                file_path=file_path,
                sheet_name=settings.get("module_validator_lpu.budget_data.sheet_name", "Tables"),
            ),
            columns_to_upper=True,
            cells_to_upper=True,
            cells_to_remove_spaces=settings.get("module_validator_lpu.budget_data.cells_to_remove_spaces", []),
            cells_to_remove_accents=settings.get("module_validator_lpu.budget_data.cells_to_remove_accents", []),
        )
    except Exception as e:
        raise ValidatorLPUError(f"Erro ao carregar or√ßamento: {e}")

    # Valida colunas obrigat√≥rias
    empty_columns = set(required_columns.keys()) - set(df.columns)
    if empty_columns:
        raise MissingColumnsError(
            f"Colunas obrigat√≥rias ausentes no or√ßamento: {', '.join(empty_columns)}"
        )

    # Garante tipos corretos usando cast_columns
    try:
        df = cast_columns(df, required_columns)
    except ValueError as e:
        raise ValidatorLPUError(f"Erro ao converter tipos de colunas: {e}")

    # Se total_orcado n√£o existir, calcula
    df = calculate_total_item(
        df=df,
        column_total_value=column_total_value,
        column_quantity=settings.get("module_validator_lpu.budget_data.column_quantity", "qtde"),
        column_unit_price=settings.get(
            "module_validator_lpu.budget_data.column_unit_price", "unitario_orcado"
        ),
    )

    return df


def load_metadata(file_path: Union[str, Path] = None) -> pd.DataFrame:
    """
    Carrega o arquivo de metadados.

    Args:
        file_path: Caminho para o arquivo de metadados (Excel ou CSV). Se n√£o for fornecido, usa o caminho padr√£o.

    Returns:
        DataFrame com a base de metadados carregada.

    Raises:
        FileNotFoundError: Se o arquivo n√£o for encontrado.
        MissingColumnsError: Se colunas obrigat√≥rias estiverem ausentes.
        ValueError: Se houver erro ao converter os tipos de colunas.
    """
    
    logger.info("Iniciando o carregamento da base de metadados")
    
    # Obt√©m o caminho e a aba do arquivo a partir das configura√ß√µes
    file_path = file_path or settings.get("module_validator_lpu.budget_metadados.file_path")
    sheet_name = settings.get("module_validator_lpu.budget_metadados.sheet_name", "Metadata")

    # Colunas obrigat√≥rias e seus tipos
    required_columns = settings.get(
        "module_validator_lpu.budget_metadados.required_columns_with_types", {}
    )

    file_path = Path(file_path)

    # Verifica se o arquivo existe
    if not file_path.exists():
        raise FileNotFoundError(f"Arquivo n√£o encontrado: {file_path}")

    try:
        # L√™ os dados e realiza o pr√©-processamento
        df = transform_case(
            read_data(file_path=file_path, sheet_name=sheet_name),
            columns_to_upper=True,
            cells_to_upper=True,
            cells_to_remove_spaces=settings.get("module_validator_lpu.budget_metadados.cells_to_remove_spaces", []),
            cells_to_remove_accents=settings.get("module_validator_lpu.budget_metadados.cells_to_remove_accents", []),
        )

        # Converte as colunas para os tipos corretos
        df = cast_columns(df, required_columns)
    except ValueError as e:
        logger.error(f"Erro ao converter tipos de colunas na base de metadados: {e}")
        raise

    return df


class LPUFormatReport(NamedTuple):
    """
    Representa o formato detectado de uma base LPU (Lista de Pre√ßos Unit√°rios).

    Attributes:
        format (str): O formato detectado da base, podendo ser "wide", "long" ou "unknown".
    """

    format: str
    columns: list = []

    def __str__(self):
        return f"LPUFormatReport(format={self.format}, columns={self.columns})"


def identify_lpu_format(
    df: pd.DataFrame,
    *,
    expected_core_cols: Tuple[str, str, str] = ("C√ìD ITEM", "ITEM", "UN"),
    long_required_cols: Tuple[str, str, str] = ("REGIAO", "GRUPO", "PRECO"),
) -> "LPUFormatReport":
    """
    Identifica se a base est√° no formato:
      - wide: colunas de pre√ßo por REGIAO/GRUPO (ex: 'NORTE-GRUPO1', ...)
      - long: colunas expl√≠citas 'regiao', 'grupo', 'preco' (nomes flex√≠veis)
      - unknown: n√£o d√° pra inferir com confian√ßa

    Args:
        df (pd.DataFrame): DataFrame a ser analisado.
        expected_core_cols (Tuple[str, str, str]): Colunas principais esperadas no DataFrame.
        long_required_cols (Tuple[str, str, str]): Colunas esperadas no formato long.

    Returns:
        LPUFormatReport: Relat√≥rio contendo o formato identificado e as colunas encontradas.

    Observa√ß√£o:
        O argumento `col_to_regiao_grupo` pode ser enviado como `report.columns` para reutilizar as colunas detectadas.
    """
    # Colunas esperadas no formato "wide"
    regions = settings.get("module_validator_lpu.lpu_data.regions", [])
    groups = settings.get("module_validator_lpu.lpu_data.groups", [])
    expected_wide_cols = generate_region_group_combinations(
        regions, groups, combine_regions=False
    ) + generate_region_group_combinations(regions, groups, combine_regions=True)

    # Identifica colunas que seguem o padr√£o de regi√£o-grupo
    found_wide_cols = [col for col in df.columns if col in expected_wide_cols]

    # Verifica se todas as colunas principais est√£o presentes
    if all(col in df.columns for col in expected_core_cols):
        # Se as colunas de pre√ßo seguem o padr√£o esperado, √© wide
        if found_wide_cols:
            return LPUFormatReport(format="wide", columns=found_wide_cols)
        # Se as colunas 'regiao', 'grupo' e 'preco' est√£o presentes, √© long
        elif all(col in df.columns for col in long_required_cols):
            return LPUFormatReport(
                format="long",
                columns=settings.get(
                    "module_validator_lpu.lpu_data.long_format_columns", {}
                ).keys(),
            )

    # Se chegou aqui, o formato √© desconhecido
    return LPUFormatReport(format="unknown", columns=[])


def wide_to_long(
    df_wide: pd.DataFrame,
    id_col: str = "C√ìD ITEM",
    item_col: str = "ITEM",
    unit_col: str = "UN",
    keep_cols: Optional[List[str]] = None,
    col_to_regiao_grupo: List[str] = None,  # Agora espera uma lista de colunas j√° filtradas
    value_name: str = "preco",
) -> pd.DataFrame:
    """
    Converte uma base LPU no formato WIDE para LONG.

    No formato WIDE, os pre√ßos s√£o organizados em colunas que representam combina√ß√µes de regi√µes e grupos,
    como 'NORTE-GRUPO1', 'SUDESTE-GRUPO2', etc. No formato LONG, essas informa√ß√µes s√£o transformadas em
    linhas, com colunas expl√≠citas para 'regiao', 'grupo' e 'preco'.

    Args:
        df_wide (pd.DataFrame): DataFrame no formato WIDE.
        id_col (str): Nome da coluna que identifica o item (ex.: 'C√ìD ITEM').
        item_col (str): Nome da coluna que descreve o item (ex.: 'ITEM').
        unit_col (str): Nome da coluna que indica a unidade (ex.: 'UN').
        keep_cols (Optional[List[str]]): Lista de colunas adicionais a serem mantidas no formato LONG.
        col_to_regiao_grupo (List[str]): Lista de colunas de regi√£o/grupo j√° filtradas no DataFrame.
        value_name (str): Nome da coluna que conter√° os valores (ex.: 'preco').

    Returns:
        pd.DataFrame: DataFrame convertido para o formato LONG.

    Raises:
        ValueError: Se as colunas necess√°rias n√£o forem encontradas ou se a convers√£o falhar.
    """

    # Definindo as regi√µes e grupos esperados
    regions = settings.get("module_validator_lpu.lpu_data.regions", [])
    groups = settings.get("module_validator_lpu.lpu_data.groups", [])

    # Copia o DataFrame para evitar altera√ß√µes no original
    df_wide = df_wide.copy()

    # Verifica se col_to_regiao_grupo foi fornecido
    if not col_to_regiao_grupo:
        raise ValueError(
            "A lista de colunas de regi√£o/grupo (col_to_regiao_grupo) n√£o foi fornecida."
        )

    # Transforma o DataFrame para o formato LONG usando a fun√ß√£o melt
    df_long = df_wide.melt(
        id_vars=[id_col, item_col, unit_col] + (keep_cols or []),  # Colunas que permanecem fixas
        value_vars=col_to_regiao_grupo,  # Colunas que ser√£o transformadas em linhas
        var_name="regiao_grupo",  # Nome da coluna que conter√° os nomes das colunas originais
        value_name=value_name,  # Nome da coluna que conter√° os valores
    )

    # Divide regiao_grupo em 'regiao' e 'grupo' usando a fun√ß√£o split_regiao_grupo
    df_long["regiao"], df_long["grupo"] = zip(
        *df_long["regiao_grupo"].apply(lambda col: split_regiao_grupo(col, regions, groups))
    )

    # Retorna o DataFrame no formato LONG
    return df_wide, df_long


def long_to_wide(
    df_long: pd.DataFrame,
    *,
    id_col: str = "cod_item",
    item_col: str = "item",
    unit_col: str = "unidade",
    regiao_col: str = "regiao",
    grupo_col: str = "grupo",
    value_col: str = "preco",
    wide_col_formatter: Optional[callable] = None,
    aggfunc: str = "first",
) -> pd.DataFrame:
    """
    Converte LPU LONG -> WIDE.
    """
    # Cria coluna √∫nica para regi√£o-grupo se n√£o existir
    if regiao_col in df_long.columns and grupo_col in df_long.columns:
        df_long["regiao_grupo"] = df_long[regiao_col] + "-" + df_long[grupo_col]
    else:
        df_long["regiao_grupo"] = df_long.get(regiao_col, df_long.get(grupo_col))

    # Agrega dados se necess√°rio
    df_agg = (
        df_long.groupby([id_col, item_col, unit_col, "regiao_grupo"])
        .agg({value_col: aggfunc})
        .reset_index()
    )

    # Transforma para formato largo
    df_wide = df_agg.pivot_table(
        index=[id_col, item_col, unit_col],
        columns="regiao_grupo",
        values=value_col,
        aggfunc=aggfunc,
    ).reset_index()

    # Formata colunas largas se fun√ß√£o fornecida
    if wide_col_formatter:
        df_wide.columns = [
            wide_col_formatter(col) if col not in [id_col, item_col, unit_col] else col
            for col in df_wide.columns
        ]

    return df_wide


def convert_lpu(
    df: pd.DataFrame,
    target: Literal["wide", "long"],
    *,
    detect: bool = True,
    **kwargs,
) -> pd.DataFrame:
    """
    Converte a LPU para o formato desejado.
    """
    if detect:
        report = identify_lpu_format(df)
        if report.format == "wide" and target == "long":
            df_wide, df_long = wide_to_long(df, col_to_regiao_grupo=report.columns, **kwargs)
        elif report.format == "long" and target == "wide":
            df_wide, df_long = long_to_wide(df, **kwargs)
        else:
            raise ValidatorLPUError(
                f"Convers√£o de {report.format} para {target} n√£o suportada ou formato desconhecido."
            )
    else:
        if target == "long":
            df_wide, df_long = wide_to_long(df, **kwargs)
        elif target == "wide":
            df_wide, df_long = long_to_wide(df, **kwargs)
        else:
            raise ValidatorLPUError(f"Formato alvo desconhecido: {target}")

    return df


def load_lpu(file_path: Union[str, Path]) -> pd.DataFrame:
    """
    Carrega o arquivo base da LPU.

    Args:
        file_path: Caminho para o arquivo da LPU

    Returns:
        DataFrame com a base da LPU carregada

    Raises:
        FileNotFoundError: Se o arquivo n√£o for encontrado
        MissingColumnsError: Se colunas obrigat√≥rias estiverem ausentes
    """
    
    logger.info("Iniciando o carregamento da base LPU")
    
    file_path = Path(file_path)

    if not file_path.exists():
        raise FileNotFoundError(f"Arquivo LPU n√£o encontrado: {file_path}")

    # Colunas obrigat√≥rias
    required_columns = settings.get(
        "module_validator_lpu.lpu_data.required_columns_with_types",
        [
            "C√ìD ITEM",
            "ITEM",
            "UN",
        ],
    )

    try:
        # Ler os dados de LPU e realizar pr√© processing
        df = transform_case(
            read_data(
                file_path=file_path,
                sheet_name=settings.get("module_validator_lpu.lpu_data.sheet_name", "LPU"),
            ),
            columns_to_upper=True,
            cells_to_upper=True,
            cells_to_remove_spaces=settings.get("module_validator_lpu.lpu_data.cells_to_remove_spaces", []),
            cells_to_remove_accents=settings.get("module_validator_lpu.lpu_data.cells_to_remove_accents", []),
        )
    except Exception as e:
        raise ValidatorLPUError(f"Erro ao carregar base LPU: {e}")

    # Valida colunas obrigat√≥rias na base de LPU
    missing_columns = set(required_columns) - set(df.columns)
    if missing_columns:
        raise MissingColumnsError(
            f"Colunas obrigat√≥rias ausentes na LPU: {', '.join(missing_columns)}"
        )

    # Detecta formato e converte se necess√°rio
    report = identify_lpu_format(df)
    if report.format == "wide":
        df_wide, df_long = wide_to_long(df, col_to_regiao_grupo=report.columns)
        logger.info("Convertido LPU de WIDE para LONG.")
    elif report.format == "long":
        # df = long_to_wide(df)
        logger.info("Mantido LPU no formato LONG.")
    else:
        raise ValidatorLPUError(f"Formato desconhecido: {report.format}")

    try:
        # Adiciona as colunas detectadas ao required_columns
        required_columns.update({column: float for column in report.columns})

        # Converter as colunas do dataframe para os tipos corretos
        df_wide = cast_columns(df_wide, required_columns)
    except ValueError as e:
        raise ValidatorLPUError(f"Erro ao converter tipos de colunas: {e}")

    return df_wide


def load_agencies(file_path: Union[str, Path]) -> pd.DataFrame:
    """
    Carrega o arquivo de ag√™ncias.

    Args:
        file_path: Caminho para o arquivo de ag√™ncias (Excel ou CSV).

    Returns:
        DataFrame com a base de ag√™ncias carregada.

    Raises:
        FileNotFoundError: Se o arquivo n√£o for encontrado.
        MissingColumnsError: Se colunas obrigat√≥rias estiverem ausentes.
    """
    
    logger.info("Iniciando o carregamento da base de ag√™ncias")
    
    file_path = Path(file_path)

    if not file_path.exists():
        raise FileNotFoundError(f"Arquivo n√£o encontrado: {file_path}")

    # Colunas obrigat√≥rias
    required_columns = settings.get(
        "module_validator_lpu.agencies_data.required_columns_with_types", []
    )

    try:
        # Ler os dados de Ag√™ncias e realizar pr√© processing
        df = transform_case(
            read_data(
                file_path=file_path,
                sheet_name=settings.get("module_validator_lpu.agencies_data.sheet_name", "Sheet1"),
            ),
            columns_to_upper=True,
            cells_to_upper=True,
            cells_to_remove_spaces=settings.get("module_validator_lpu.agencies_data.cells_to_remove_spaces", []),
            cells_to_remove_accents=settings.get("module_validator_lpu.agencies_data.cells_to_remove_accents", []),
        )
    except Exception as e:
        logger.error(f"Erro ao carregar o arquivo de ag√™ncias: {e}")
        raise

    # Valida colunas obrigat√≥rias
    missing_columns = set(required_columns.keys()) - set(df.columns)
    if missing_columns:
        raise MissingColumnsError(
            f"Colunas obrigat√≥rias ausentes na base de ag√™ncias: {missing_columns}"
        )

    # Garante tipos corretos usando cast_columns
    try:
        df = cast_columns(df, required_columns)
    except ValueError as e:
        logger.error(f"Erro ao converter tipos de colunas na base de ag√™ncias: {e}")
        raise

    return df


def load_constructors(file_path: Union[str, Path]) -> pd.DataFrame:
    """
    Carrega o arquivo de construtoras.

    Args:
        file_path: Caminho para o arquivo de construtoras (Excel ou CSV).

    Returns:
        DataFrame com a base de construtoras carregada.

    Raises:
        FileNotFoundError: Se o arquivo n√£o for encontrado.
        MissingColumnsError: Se colunas obrigat√≥rias estiverem ausentes.
    """
    
    logger.info("Iniciando o carregamento da base de construtoras")
    
    file_path = Path(file_path)

    if not file_path.exists():
        raise FileNotFoundError(f"Arquivo n√£o encontrado: {file_path}")

    # Colunas obrigat√≥rias
    required_columns = settings.get(
        "module_validator_lpu.constructors_data.required_columns_with_types", []
    )

    try:
        # Ler os dados de Fornecedores/Construtoras e realizar pr√© processing
        df = transform_case(
            read_data(
                file_path=file_path,
                sheet_name=settings.get("module_validator_lpu.constructors_data.sheet_name", "Sheet1"),
            ),
            columns_to_upper=True,
            cells_to_upper=True,
            cells_to_remove_spaces=settings.get("module_validator_lpu.constructors_data.cells_to_remove_spaces", []),
            cells_to_remove_accents=settings.get("module_validator_lpu.constructors_data.cells_to_remove_accents", []),
        )
    except Exception as e:
        logger.error(f"Erro ao carregar o arquivo de construtoras: {e}")
        raise

    # Valida colunas obrigat√≥rias
    missing_columns = set(required_columns.keys()) - set(df.columns)
    if missing_columns:
        raise MissingColumnsError(
            f"Colunas obrigat√≥rias ausentes na base de construtoras: {missing_columns}"
        )

    # Garante tipos corretos usando cast_columns
    try:
        df = cast_columns(df, required_columns)
    except ValueError as e:
        logger.error(f"Erro ao converter tipos de colunas na base de construtoras: {e}")
        raise

    return df


def calculate_total_item(
    df: pd.DataFrame, column_total_value: str, column_quantity: str, column_unit_price: str
) -> pd.DataFrame:
    """
    Calcula o valor total or√ßado em um DataFrame.

    Args:
        df (pd.DataFrame): DataFrame contendo os dados.
        column_total_value (str): Nome da coluna de valor total or√ßado.
        column_quantity (str): Nome da coluna de quantidade.
        column_unit_price (str): Nome da coluna de pre√ßo unit√°rio.

    Returns:
        pd.DataFrame: DataFrame atualizado com a coluna de valor total or√ßado calculada ou convertida.
    """
    if column_total_value not in df.columns:
        df[column_total_value] = df[column_quantity] * df[column_unit_price]
    else:
        df[column_total_value] = pd.to_numeric(df[column_total_value], errors="coerce")

    return df


def get_default_settings(key):
    """
    Retorna os valores padr√£o das configura√ß√µes do validador LPU.

    Returns:
        Dicion√°rio com configura√ß√µes padr√£o
    """
    return {
        "default_budget_path": settings.validador_lpu.caminho_padrao_orcamento,
        "default_lpu_path": settings.validador_lpu.caminho_padrao_lpu,
        "output_dir": settings.validador_lpu.output_dir,
        "tolerance_percentual": settings.validador_lpu.tolerancia_percentual,
        "basic_excel_file": settings.validador_lpu.arquivo_excel_basico,
        "complete_excel_file": settings.validador_lpu.arquivo_excel_completo,
        "csv_file": settings.validador_lpu.arquivo_csv,
        "html_file": settings.validador_lpu.arquivo_html,
        "top_n_divergences": settings.validador_lpu.top_n_divergencias,
        "top_n_divergences_extended": settings.validador_lpu.top_n_divergencias_extended,
    }


def validate_and_merge(
    df_left: pd.DataFrame,
    df_right: pd.DataFrame,
    left_on: List[str],
    right_on: List[str],
    how: str = "left",
    log_message: str = "",
) -> Tuple[pd.DataFrame, int, int]:
    """
    Fun√ß√£o gen√©rica para validar tipos, realizar merge e contar os resultados.

    Args:
        df_left (pd.DataFrame): DataFrame √† esquerda.
        df_right (pd.DataFrame): DataFrame √† direita.
        left_on (List[str]): Colunas do DataFrame √† esquerda para o merge.
        right_on (List[str]): Colunas do DataFrame √† direita para o merge.
        how (str): Tipo de merge (ex.: 'left', 'inner').
        log_message (str): Mensagem de log para identificar o merge.

    Returns:
        Tuple[pd.DataFrame, int, int]:
            - DataFrame resultante do merge.
            - Quantidade de itens que deram match.
            - Quantidade total de itens no DataFrame √† esquerda.
    """
    # Valida se os tipos das colunas s√£o compat√≠veis
    for col_left, col_right in zip(left_on, right_on):
        if df_left[col_left].dtype != df_right[col_right].dtype:
            logger.warning(
                f"‚ö†Ô∏è Tipos diferentes nas colunas: {col_left} ({df_left[col_left].dtype}) e {col_right} ({df_right[col_right].dtype})."
            )

    # Realiza o merge
    merged_df = pd.merge(
        df_left,
        df_right,
        left_on=left_on,
        right_on=right_on,
        how=how,
        suffixes=("_orc", "_lpu"),
        indicator=True,
    )

    # Conta os itens que deram match
    matched_count = merged_df[merged_df["_merge"] == "both"].shape[0]
    total_count = df_left.shape[0]

    # Loga a informa√ß√£o do merge
    logger.info(
        f"{log_message} - {matched_count} dados de {total_count} ({round((matched_count / total_count) * 100, 2) if total_count > 0 else 0}%)"
    )

    return merged_df, matched_count, total_count


def merge_budget_lpu(
    df_budget: pd.DataFrame,
    df_lpu: pd.DataFrame,
    columns_on_budget: List[str],
    columns_on_lpu: List[str],
    secondary_columns_on_budget: Optional[List[str]] = None,
    secondary_columns_on_lpu: Optional[List[str]] = None,
) -> pd.DataFrame:
    """
    Mescla or√ßamento e LPU usando colunas especificadas, com fallback para colunas secund√°rias.

    Args:
        df_budget: DataFrame do or√ßamento.
        df_lpu: DataFrame da base LPU.
        columns_on_budget: Colunas prim√°rias do df_budget para usar na mesclagem.
        columns_on_lpu: Colunas prim√°rias do df_lpu para usar na mesclagem.
        secondary_columns_on_budget: Colunas secund√°rias do df_budget para fallback.
        secondary_columns_on_lpu: Colunas secund√°rias do df_lpu para fallback.

    Returns:
        DataFrame combinado com INNER JOIN.

    Raises:
        ValidatorLPUError: Se a mesclagem resultar em um DataFrame vazio.
    """
    # Realiza o primeiro merge (colunas prim√°rias)
    merged_df, matched_count, total_count = validate_and_merge(
        df_budget,
        df_lpu,
        columns_on_budget,
        columns_on_lpu,
        how="left",
        log_message=f"Match realizado usando as colunas prim√°rias: {columns_on_budget} - {columns_on_lpu}",
    )

    # Filtra os itens que n√£o foram cruzados no primeiro merge
    not_matched = merged_df[merged_df["_merge"] == "left_only"].drop(columns=["_merge"])

    # Se houver itens n√£o cruzados e colunas secund√°rias forem fornecidas, tenta o merge secund√°rio
    if not not_matched.empty and secondary_columns_on_budget and secondary_columns_on_lpu:
        # Realiza o segundo merge (colunas secund√°rias)
        secondary_merge, matched_secondary_count, not_matched_count = validate_and_merge(
            not_matched,
            df_lpu,
            secondary_columns_on_budget,
            secondary_columns_on_lpu,
            how="left",
            log_message=f"Segundo match realizado usando as colunas secund√°rias: {secondary_columns_on_budget} - {secondary_columns_on_lpu}",
        )

        # Atualiza os itens cruzados e n√£o cruzados
        matched_secondary = secondary_merge[secondary_merge["_merge"] == "both"].drop(
            columns=["_merge"]
        )
        not_matched_secondary = secondary_merge[secondary_merge["_merge"] == "left_only"].drop(
            columns=["_merge"]
        )

        # Concatena os resultados do primeiro e segundo cruzamento
        merged_df = pd.concat(
            [merged_df[merged_df["_merge"] == "both"], matched_secondary],
            ignore_index=True,
        )
        not_matched = not_matched_secondary
    logger.info(
        "Match realizado usando as colunas: {} - {} - {} dados de {} ({}%)".format(
            columns_on_budget,
            columns_on_lpu,
            matched_count,
            total_count,
            round((matched_count / total_count) * 100, 2),
        )
    )

    # Se ainda houver itens n√£o cruzados, adiciona uma coluna de status
    if not not_matched.empty:
        not_matched["status"] = "N√£o cruzado"
        logger.warning(f"‚ö†Ô∏è {len(not_matched)} itens n√£o foram cruzados.")

    # Retorna o DataFrame final
    return merged_df


def validate_lpu(
    file_path_budget: Union[str, Path] = None,
    file_path_metadata: Union[str, Path] = None,
    file_path_lpu: Union[str, Path] = None,
    file_path_agencies: Union[str, Path] = None,
    file_path_constructors: Union[str, Path] = None,
    output_dir: Union[str, Path] = None,
    output_file: str = "02_BASE_RESULTADO_VALIDADOR_LPU.xlsx",
    verbose: bool = True,
) -> pd.DataFrame:
    """
    Fun√ß√£o orquestradora para valida√ß√£o LPU.

    Realiza todo o fluxo de valida√ß√£o:
    1. Carrega or√ßamento, LPU, ag√™ncias e construtoras.
    2. Cruza os dados (INNER JOIN em cod_item + unidade).
    3. Calcula discrep√¢ncias com toler√¢ncia configur√°vel.
    4. Classifica itens (OK, Para ressarcimento, Abaixo LPU).
    5. Salva resultados em formatos Excel, CSV e HTML.

    Args:
        file_path_budget: Caminho para o arquivo de or√ßamento (padr√£o nas configura√ß√µes).
        file_path_metadata: Caminho para o arquivo de metadados (padr√£o nas configura√ß√µes).
        file_path_lpu: Caminho para o arquivo da LPU (padr√£o nas configura√ß√µes).
        file_path_agencies: Caminho para o arquivo de ag√™ncias (padr√£o nas configura√ß√µes).
        file_path_constructors: Caminho para o arquivo de construtoras (padr√£o nas configura√ß√µes).
        output_dir: Diret√≥rio para salvar resultados (padr√£o nas configura√ß√µes).
        output_file_name: Nome base para os arquivos de sa√≠da (sem extens√£o).
        verbose: Se True, exibe estat√≠sticas no console.

    Returns:
        DataFrame com os resultados completos da valida√ß√£o.

    Raises:
        ValidatorLPUError: Em caso de erro na valida√ß√£o.
    """

    if verbose:
        print("-" * 50)
        logger.info("VALIDADOR LPU - Concilia√ß√£o Or√ßamento vs Base de Pre√ßos")
        logger.info(
            f"Toler√¢ncia configurada: {settings.get('module_validator_lpu.tol_percentile')}%"
        )
        print("-" * 50)

    # 1. Carrega dados
    if verbose:
        logger.info("üìÇ Carregando arquivos...")

    try:
        logger.info(f"Carregando or√ßamento de: {file_path_budget}")
        df_budget = load_budget(file_path_budget)
        if verbose:
            logger.info(f"   ‚úÖ Or√ßamento carregado: {len(df_budget)} itens")
    except Exception as e:
        logger.error(f"Erro ao carregar or√ßamento: {e}")
        raise ValidatorLPUError(f"Erro ao carregar or√ßamento: {e}")

    try:
        logger.info(f"Carregando metadados de or√ßamentos de: {file_path_metadata}")
        df_budget_metadata = load_metadata(file_path_metadata)
        if verbose:
            logger.info(
                f"   ‚úÖ Metadados dos or√ßamentos carregado: {len(df_budget_metadata)} itens"
            )
    except Exception as e:
        logger.error(f"Erro ao carregar metadados dos or√ßamentos: {e}")
        raise ValidatorLPUError(f"Erro ao carregar metadados dos or√ßamentos: {e}")

    try:
        logger.debug(f"Carregando LPU de: {file_path_lpu}")
        df_lpu = load_lpu(file_path_lpu)
        if verbose:
            logger.info(f"   ‚úÖ LPU carregada: {len(df_lpu)} itens")
    except Exception as e:
        logger.error(f"Erro ao carregar LPU: {e}")
        raise ValidatorLPUError(f"Erro ao carregar LPU: {e}")

    try:
        logger.debug(f"Carregando ag√™ncias de: {file_path_agencies}")
        df_agencies = load_agencies(file_path_agencies)
        if verbose:
            logger.info(f"   ‚úÖ Ag√™ncias carregadas: {len(df_agencies)} itens")
    except Exception as e:
        logger.error(f"Erro ao carregar ag√™ncias: {e}")
        raise ValidatorLPUError(f"Erro ao carregar ag√™ncias: {e}")

    try:
        logger.debug(f"Carregando construtoras de: {file_path_constructors}")
        df_constructors = load_constructors(file_path_constructors)
        if verbose:
            logger.info(f"   ‚úÖ Construtoras carregadas: {len(df_constructors)} itens")
    except Exception as e:
        logger.error(f"Erro ao carregar construtoras: {e}")
        raise ValidatorLPUError(f"Erro ao carregar construtoras: {e}")

    # 2. Cruza dados
    if verbose:
        logger.info("üîó Cruzando or√ßamento com LPU...")
    
    try:
        # Realiza o merge entre budget e metadados
        df_merge_budget_metadata = merge_data_with_columns(
            df_left=df_budget,
            df_right=df_budget_metadata,
            left_on=settings.get("module_validator_lpu.merge_budget_metadata.left_on"),
            right_on=settings.get("module_validator_lpu.merge_budget_metadata.right_on"),
            how=settings.get("module_validator_lpu.merge_budget_metadata.how", "left"),
            suffixes=("_orc", "_meta"),
            validate=settings.get("module_validator_lpu.merge_budget_metadata.validate", "many_to_one"),
        )
        if verbose:
            logger.info(f"   ‚úÖ Itens cruzados: {len(df_merge_budget_metadata)}")
            logger.info(f"   ‚úÖ Qtd de linhas e colunas: {df_merge_budget_metadata.shape}")
    except Exception as e:
        logger.error(f"Erro ao cruzar dados: {e}")
        raise ValidatorLPUError(f"Erro ao cruzar dados: {e}")
    
    try:
        # Realiza o merge entre budget/metadados e agencias
        df_merge_budget_metadata_agencias = merge_data_with_columns(
            df_left=df_merge_budget_metadata,
            df_right=df_agencies,
            left_on=settings.get("module_validator_lpu.merge_budget_metadata_agencies.left_on"),
            right_on=settings.get("module_validator_lpu.merge_budget_metadata_agencies.right_on"),
            how=settings.get("module_validator_lpu.merge_budget_metadata_agencies.how", "left"),
            suffixes=("_meta", "_age"),
            validate=settings.get("module_validator_lpu.merge_budget_metadata_agencies.validate", "many_to_one"),
        )
        if verbose:
            logger.info(f"   ‚úÖ Itens cruzados: {len(df_merge_budget_metadata_agencias)}")
            logger.info(f"   ‚úÖ Qtd de linhas e colunas: {df_merge_budget_metadata_agencias.shape}")
    except Exception as e:
        logger.error(f"Erro ao cruzar dados: {e}")
        raise ValidatorLPUError(f"Erro ao cruzar dados: {e}")
    
    try:
        # Realiza o merge entre budget/metadados e agencias
        df_merge_budget_metadata_agencias_constructors = merge_data_with_columns(
            df_left=df_merge_budget_metadata_agencias,
            df_right=df_constructors,
            left_on=settings.get("module_validator_lpu.merge_budget_metadata_agencies_constructors.left_on"),
            right_on=settings.get("module_validator_lpu.merge_budget_metadata_agencies_constructors.right_on"),
            how=settings.get("module_validator_lpu.merge_budget_metadata_agencies_constructors.how", "left"),
            suffixes=("_age", "_constr"),
            validate=settings.get("module_validator_lpu.merge_budget_metadata_agencies_constructors.validate", "many_to_one"),
        )
        if verbose:
            logger.info(f"   ‚úÖ Itens cruzados: {len(df_merge_budget_metadata_agencias_constructors)}")
            logger.info(f"   ‚úÖ Qtd de linhas e colunas: {df_merge_budget_metadata_agencias_constructors.shape}")
    except Exception as e:
        logger.error(f"Erro ao cruzar dados: {e}")
        raise ValidatorLPUError(f"Erro ao cruzar dados: {e}")
    
    # Salvar o resultado em um arquivo Excel
    export_data(data=df_merge_budget_metadata_agencias_constructors, 
                file_path=Path(output_dir, output_file), 
                index=False)
    logger.success(f"Resultado salvo em: {output_file}")

    try:
        # Realiza o merge entre or√ßamento e LPU
        df_merge_budget_metadata_agencias_constructors_lpu = merge_budget_lpu(
            df_budget=df_merge_budget_metadata_agencias_constructors,
            df_lpu=df_lpu,
            columns_on_budget=[
                settings.get("module_validator_lpu.merge_budget_lpu.columns_on_budget_id"),
            ],  # Coluna prim√°ria do df_budget
            columns_on_lpu=[
                settings.get("module_validator_lpu.merge_budget_lpu.columns_on_lpu_id"),
            ],  # Coluna prim√°ria do df_lpu
            secondary_columns_on_budget=[
                settings.get("module_validator_lpu.merge_budget_lpu.columns_on_budget_nome"),
            ],  # Coluna secund√°ria do df_budget
            secondary_columns_on_lpu=[
                settings.get("module_validator_lpu.merge_budget_lpu.columns_on_lpu_nome"),
            ],  # Coluna secund√°ria do df_lpu
        )
        if verbose:
            logger.info(f"   ‚úÖ Itens cruzados: {len(df_merge_budget_metadata_agencias_constructors_lpu)}")
    except Exception as e:
        logger.error(f"Erro ao cruzar dados: {e}")
        raise ValidatorLPUError(f"Erro ao cruzar dados: {e}")

    if verbose:
        logger.info("")

    # 3. Calcula discrep√¢ncias
    if verbose:
        logger.info(
            f"üßÆ Calculando discrep√¢ncias (toler√¢ncia {settings.get('validador_lpu.tolerancia_percentual')}%)..."
        )

    try:
        df_result = calculate_discrepancies(df_budget_lpu)
    except Exception as e:
        logger.error(f"Erro ao calcular discrep√¢ncias: {e}")
        raise ValidatorLPUError(f"Erro ao calcular discrep√¢ncias: {e}")

    # Estat√≠sticas
    if verbose:
        logger.info("")
        logger.info("üìä ESTAT√çSTICAS DA VALIDA√á√ÉO")
        logger.info("-" * 80)

        total_items = len(df_result)
        items_ok = (df_result["status_conciliacao"] == "OK").sum()
        items_refund = (df_result["status_conciliacao"] == "Para ressarcimento").sum()
        items_below = (df_result["status_conciliacao"] == "Abaixo LPU").sum()

        logger.info(f"   Total de itens validados: {total_items}")
        logger.info(f"   ‚úÖ OK: {items_ok} ({items_ok/total_items*100:.1f}%)")
        logger.info(
            f"   ‚ö†Ô∏è  Para ressarcimento: {items_refund} ({items_refund/total_items*100:.1f}%)"
        )
        logger.info(f"   üìâ Abaixo LPU: {items_below} ({items_below/total_items*100:.1f}%)")
        logger.info("")

        total_budgeted_value = df_result["valor_total_orcado"].sum()
        total_divergence = df_result["dif_total"].sum()
        refund_divergence = df_result[df_result["status_conciliacao"] == "Para ressarcimento"][
            "dif_total"
        ].sum()

        logger.info(f"   üí∞ Valor total or√ßado: R$ {total_budgeted_value:,.2f}")
        logger.info(f"   üíµ Diverg√™ncia total: R$ {total_divergence:,.2f}")
        logger.info(f"   üí∏ Ressarcimento potencial: R$ {refund_divergence:,.2f}")
        logger.info("")

    # Registra estat√≠sticas no logger
    logger.debug("üìä ESTAT√çSTICAS DA VALIDA√á√ÉO")
    total_items = len(df_result)
    items_ok = (df_result["status_conciliacao"] == "OK").sum()
    items_refund = (df_result["status_conciliacao"] == "Para ressarcimento").sum()
    items_below = (df_result["status_conciliacao"] == "Abaixo LPU").sum()

    logger.debug(f"Total de itens validados: {total_items}")
    logger.debug(f"‚úÖ OK: {items_ok} ({items_ok/total_items*100:.1f}%)")
    logger.debug(f"‚ö†Ô∏è  Para ressarcimento: {items_refund} ({items_refund/total_items*100:.1f}%)")
    logger.debug(f"üìâ Abaixo LPU: {items_below} ({items_below/total_items*100:.1f}%)")

    total_budgeted_value = df_result["valor_total_orcado"].sum()
    total_divergence = df_result["dif_total"].sum()
    refund_divergence = df_result[df_result["status_conciliacao"] == "Para ressarcimento"][
        "dif_total"
    ].sum()

    logger.debug(f"üí∞ Valor total or√ßado: R$ {total_budgeted_value:,.2f}")
    logger.debug(f"üíµ Diverg√™ncia total: R$ {total_divergence:,.2f}")
    logger.debug(f"üí∏ Ressarcimento potencial: R$ {refund_divergence:,.2f}")

    # 4. Salva resultados
    if verbose:
        logger.info("")
        logger.info("üíæ Salvando resultados...")

    try:
        # Salva formato b√°sico (4 planilhas)
        save_results(df_result, output_dir)

        # Salva relat√≥rio completo em Excel (11+ planilhas)
        generate_complete_excel_report(df_result, output_dir)

        # Salva relat√≥rio HTML
        generate_html_report(df_result, output_dir)

    except Exception as e:
        logger.error(f"Erro ao salvar resultados: {e}")
        raise ValidatorLPUError(f"Erro ao salvar resultados: {e}")

    if verbose:
        logger.info("")
        logger.info("=" * 80)
        logger.success("‚úÖ VALIDA√á√ÉO CONCLU√çDA COM SUCESSO!")
        logger.info("=" * 80)

    logger.debug("=" * 80)
    logger.success("‚úÖ VALIDA√á√ÉO CONCLU√çDA COM SUCESSO!")
    logger.debug("=" * 80)

    return df_result


def orchestrate_validate_lpu(
    file_path_budget: Union[str, Path] = None,
    file_path_metadata: Union[str, Path] = None,
    file_path_lpu: Union[str, Path] = None,
    file_path_agencies: Union[str, Path] = None,
    file_path_constructors: Union[str, Path] = None,
    output_dir: Union[str, Path] = None,
    output_file: str = None,
    verbose: bool = True,
) -> int:
    """
    Fun√ß√£o principal para execu√ß√£o direta do m√≥dulo ou chamada externa.

    Args:
        file_path_budget: Caminho para o arquivo de or√ßamento (padr√£o nas configura√ß√µes se None).
        file_path_metadata: Caminho para o arquivo de metadados dos or√ßamentos (padr√£o nas configura√ß√µes se None).
        file_path_lpu: Caminho para o arquivo da LPU (padr√£o nas configura√ß√µes se None).
        file_path_agencies: Caminho para o arquivo de ag√™ncias (padr√£o nas configura√ß√µes se None).
        file_path_constructors: Caminho para o arquivo de construtoras (padr√£o nas configura√ß√µes se None).
        output_dir: Diret√≥rio para salvar resultados (padr√£o nas configura√ß√µes se None).
        output_file: Nome base para os arquivos de sa√≠da (padr√£o nas configura√ß√µes se None).
        verbose: Se True, exibe estat√≠sticas no console.

    Returns:
        int: C√≥digo de status (0 para sucesso, 1 para erro).
    """
    # Configura caminhos padr√£o se n√£o fornecidos
    base_dir = Path(__file__).parents[5]
    path_file_budget = Path(
        base_dir,
        file_path_budget or settings.get("module_validator_lpu.budget_data.file_path"),
    )
    path_file_metadata = Path(
        base_dir,
        file_path_metadata or settings.get("module_validator_lpu.budget_metadados.file_path"),
    )
    path_file_lpu = Path(
        base_dir, file_path_lpu or settings.get("module_validator_lpu.lpu_data.file_path")
    )
    path_file_agencies = Path(
        base_dir, file_path_agencies or settings.get("module_validator_lpu.agencies_data.file_path")
    )
    path_file_constructors = Path(
        base_dir,
        file_path_constructors or settings.get("module_validator_lpu.constructors_data.file_path"),
    )
    output_dir = Path(
        base_dir, output_dir or settings.get("module_validator_lpu.output_settings.output_dir")
    )
    output_file = output_file or settings.get(
        "module_validator_lpu.output_settings.file_path_output"
    )

    logger.debug(f"Or√ßamento: {path_file_budget}")
    logger.debug(f"LPU: {path_file_lpu}")
    logger.debug(f"Ag√™ncias: {path_file_agencies}")
    logger.debug(f"Construtoras: {path_file_constructors}")
    logger.debug(f"Sa√≠da: {output_dir}")

    try:
        df_result = validate_lpu(
            file_path_budget=path_file_budget,
            file_path_metadata=path_file_metadata,
            file_path_lpu=path_file_lpu,
            file_path_agencies=path_file_agencies,
            file_path_constructors=path_file_constructors,
            output_dir=output_dir,
            output_file=output_file,
            verbose=verbose,
        )

        # Exibe primeiras linhas
        if verbose:
            logger.info("\nüìã VISUALIZA√á√ÉO DOS RESULTADOS:")
            logger.info("-" * 80)
            preview_columns = [
                "cod_item",
                "nome",
                "unidade",
                "qtde",
                "unitario_orcado",
                "unitario_lpu",
                "dif_unitario",
                "perc_dif",
                "status_conciliacao",
            ]
            preview_columns = [col for col in preview_columns if col in df_result.columns]
            logger.info(f"\n{df_result[preview_columns].head(10).to_string(index=False)}")

        logger.success("Execu√ß√£o principal conclu√≠da com sucesso!")
        return 0

    except ValidatorLPUError as e:
        logger.error(f"ERRO: {e}")
        return 1
    except Exception as e:
        logger.error(f"ERRO INESPERADO: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(orchestrate_validate_lpu())
