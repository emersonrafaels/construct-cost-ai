"""
M√≥dulo: data_io
----------------

Fornece fun√ß√µes utilit√°rias padronizadas para leitura e escrita de dados em
diferentes formatos de dados (excel, csv, parquet, etc) utilizados no ecossistema do Verificador Inteligente de
Or√ßamentos de Obras.

Este m√≥dulo abstrai a complexidade de m√∫ltiplos formatos (CSV, Excel,
Parquet, JSON, Feather, Pickle), garantindo uma interface consistente para
todas as etapas do pipeline ‚Äî desde prototipa√ß√£o local at√© uso em produ√ß√£o.

üß© Funcionalidades principais:
------------------------------

1) read_data(file_path, sheet_name=None, header=0)
   - Detecta automaticamente o m√©todo de leitura a partir da extens√£o.
   - Suporta:
       .csv, .xlsx, .xls, .json, .parquet, .feather, .pkl
   - Permite leitura de abas espec√≠ficas em arquivos Excel.
   - Utilizado por:
       ‚Ä¢ Parsing de or√ßamentos
       ‚Ä¢ Testes unit√°rios
       ‚Ä¢ Pipelines determin√≠sticos

2) export_data(data, file_path, create_dirs=True)
   - Exporta DataFrames ou m√∫ltiplos DataFrames (multi-sheet Excel).
   - Cria diret√≥rios automaticamente, quando necess√°rio.
   - Suporta:
       .csv, .xlsx, .json, .parquet, .feather, .pkl
   - Utilizado por:
       ‚Ä¢ Gera√ß√£o de relat√≥rios t√©cnicos
       ‚Ä¢ Salvamento de artefatos do verificador
       ‚Ä¢ Outputs intermedi√°rios do pipeline

üéØ Motiva√ß√£o e valor:
---------------------
- Unifica a manipula√ß√£o de dados em todo o projeto.
- Reduz duplica√ß√£o de c√≥digo em parseadores, validadores e testes.
- Facilita a troca futura de formato sem alterar o restante do pipeline.
- Padroniza I/O para rodar em ambientes diversos (local, AWS, CI/CD).


üìÅ Localiza√ß√£o:
--------------
Faz parte da camada utilit√°ria `utils/`.

"""

__author__ = "Emerson V. Rafael (emervin)"
__copyright__ = "Verificador Inteligente de Or√ßamentos de Obras"
__credits__ = ["Emerson V. Rafael", "Lucas Ken", "Clarissa Simoyama"]
__license__ = "MIT"
__version__ = "1.0.0"
__maintainer__ = "Emerson V. Rafael (emervin), Lucas Ken (kushida), Clarissa Simoyama (simoyam)"
__squad__ = "DataCraft"
__email__ = "emersonssmile@gmail.com"
__status__ = "Development"

from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
import json

import pandas as pd


def read_data(
    file_path: Union[str, Path],
    sheet_name: Optional[Union[str, int]] = None,
    header: Optional[Union[int, List[int]]] = 0,
    default_sheet: Optional[Union[str, List[str]]] = ["Sheet1", "Planilha1"],
) -> pd.DataFrame:
    """
    L√™ dados de v√°rios formatos de arquivo usando a extens√£o do arquivo para determinar o m√©todo apropriado.
    Se a aba especificada (sheet_name) n√£o existir, utiliza a aba padr√£o (default_sheet).
    
    Args:
        file_path (Union[str, Path]): Caminho para o arquivo a ser lido.
        sheet_name (Optional[Union[str, int]]): Nome ou √≠ndice da aba a ser lida (para arquivos Excel). Padr√£o √© None.
        header (Optional[Union[int, List[int]]]): N√∫mero(s) da(s) linha(s) a ser(em) usada(s) como nomes das colunas. Padr√£o √© 0.
        default_sheet (Optional[Union[str, List[str]]]): Nome ou lista de nomes das abas padr√£o a serem lidas se a aba especificada n√£o for encontrada.
    
    Returns:
        pd.DataFrame: DataFrame contendo os dados lidos.
    
    Raises:
        ValueError: Se a extens√£o do arquivo n√£o for suportada.
        FileNotFoundError: Se o arquivo n√£o existir.
    """
    file_path = Path(file_path)

    if not file_path.exists():
        raise FileNotFoundError(f"File not found: {file_path}")

    # Obtendo a extens√£o do dado recebido
    extension = file_path.suffix.lower()

    # Definindo os leitores dispon√≠veis no data functions
    readers = {
        ".csv": lambda path: pd.read_csv(path, header=header),
        ".xlsx": lambda path: pd.read_excel(path, sheet_name=sheet_name, header=header),
        ".xls": lambda path: pd.read_excel(path, sheet_name=sheet_name, header=header),
        ".xlsm": lambda path: pd.read_excel(
            path, sheet_name=sheet_name, header=header
        ),  # Added support for .xlsm files
        ".json": lambda path: pd.read_json(path),
        ".parquet": lambda path: pd.read_parquet(path),
        ".feather": lambda path: pd.read_feather(path),
        ".pkl": lambda path: pd.read_pickle(path),
    }

    reader = readers.get(extension)
    if reader is None:
        raise ValueError(f"Unsupported file extension: {extension}")

    try:
        # Tenta carregar a aba especificada
        return reader(file_path)
    except ValueError as e:
        # Tratamento espec√≠fico para erro de aba n√£o encontrada
        if "Worksheet named" in str(e) and "not found" in str(e):
            try:
                # Listar todas as abas dispon√≠veis no arquivo
                available_sheets = pd.ExcelFile(file_path).sheet_names
                if isinstance(default_sheet, str) and default_sheet in available_sheets:
                    logger.warning(
                        f"Aba '{sheet_name}' n√£o encontrada. Carregando a aba padr√£o '{default_sheet}'."
                    )
                    return pd.read_excel(file_path, sheet_name=default_sheet, header=header)
                elif isinstance(default_sheet, list):
                    for sheet in default_sheet:
                        if sheet in available_sheets:
                            logger.warning(
                                f"Aba '{sheet_name}' n√£o encontrada. Carregando a aba padr√£o '{sheet}'."
                            )
                            return pd.read_excel(file_path, sheet_name=sheet, header=header)
                raise ValueError(
                    f"Aba '{sheet_name}' n√£o encontrada no arquivo '{file_path}'. "
                    f"As abas dispon√≠veis s√£o: {available_sheets}"
                )
            except Exception as inner_e:
                raise RuntimeError(
                    f"Erro ao tentar listar as abas dispon√≠veis no arquivo '{file_path}': {str(inner_e)}"
                )
        else:
            raise e
    except Exception as e:
        raise RuntimeError(f"Erro ao ler o arquivo {file_path}: {str(e)}")


def export_data(
    data: Union[pd.DataFrame, Dict[str, pd.DataFrame]],
    file_path: Union[str, Path],
    create_dirs: bool = True,
    index: bool = False,
    **kwargs,
) -> None:
    """
    Exporta dados para v√°rios formatos, com suporte para m√∫ltiplas abas em arquivos Excel.

    Args:
        data (Union[pd.DataFrame, Dict[str, pd.DataFrame]]): DataFrame ou dicion√°rio de DataFrames para exporta√ß√£o.
        file_path (Union[str, Path]): Caminho onde o arquivo ser√° salvo.
        create_dirs (bool): Se True, cria diret√≥rios automaticamente se n√£o existirem. Default √© True.
        index (bool): Se True, inclui o √≠ndice ao salvar os dados. Default √© False.
        **kwargs: Argumentos adicionais passados para a fun√ß√£o de exporta√ß√£o do pandas.

    Raises:
        ValueError: Se a extens√£o do arquivo n√£o for suportada.
        RuntimeError: Se ocorrer um erro durante a exporta√ß√£o.
    """
    file_path = Path(file_path)

    if create_dirs:
        file_path.parent.mkdir(parents=True, exist_ok=True)

    extension = file_path.suffix.lower()

    exporters = {
        ".csv": lambda df, path: df.to_csv(path, index=index, **kwargs),
        ".xlsx": lambda df, path: (
            df.to_excel(path, index=index, **kwargs)
            if isinstance(df, pd.DataFrame)
            else (_export_multiple_sheets(df, path, index=index, **kwargs))
        ),
        ".json": lambda df, path: df.to_json(path, **kwargs),
        ".parquet": lambda df, path: df.to_parquet(path, **kwargs),
        ".feather": lambda df, path: df.to_feather(path, **kwargs),
        ".pkl": lambda df, path: df.to_pickle(path, **kwargs),
    }

    exporter = exporters.get(extension)
    if exporter is None:
        raise ValueError(f"Unsupported file extension: {extension}")

    try:
        exporter(data, file_path)
    except Exception as e:
        raise RuntimeError(f"Error exporting to {file_path}: {str(e)}")


def _export_multiple_sheets(
    data: Dict[str, pd.DataFrame], path: Union[str, Path], index: bool = False, **kwargs
):
    """
    Fun√ß√£o auxiliar para exportar m√∫ltiplas abas para um arquivo Excel.

    Args:
        data (Dict[str, pd.DataFrame]): Dicion√°rio de DataFrames para exporta√ß√£o.
        path (Union[str, Path]): Caminho para o arquivo Excel.
        index (bool): Se True, inclui o √≠ndice ao salvar os dados. Default √© False.
        **kwargs: Argumentos adicionais para pandas.to_excel.
    """
    with pd.ExcelWriter(path, engine="openpyxl") as writer:
        for sheet_name, sheet_data in data.items():
            sheet_data.to_excel(writer, sheet_name=sheet_name, index=index, **kwargs)


def transform_case(
    df: pd.DataFrame,
    to_upper: bool = True,
    columns: Union[bool, List[str]] = False,
    cells: Union[bool, List[Tuple[int, int]]] = False,
) -> pd.DataFrame:
    """
    Transforma valores de texto em um DataFrame para uppercase ou lowercase, com op√ß√µes para todas as colunas ou c√©lulas espec√≠ficas.

    Args:
        df (pd.DataFrame): DataFrame a ser transformado.
        to_upper (bool): Se True, transforma os valores para uppercase. Se False, transforma para lowercase. Default √© True.
        columns (Union[bool, List[str]]): Se True, todas as colunas ser√£o transformadas. Se lista, apenas as colunas especificadas ser√£o transformadas. Default √© False.
        cells (Union[bool, List[Tuple[int, int]]]): Se True, todas as c√©lulas ser√£o transformadas. Se lista, apenas as c√©lulas especificadas ser√£o transformadas. Default √© False.

    Returns:
        pd.DataFrame: DataFrame com os valores transformados.
    """

    def transform_value(value):
        if isinstance(value, str):
            return value.upper() if to_upper else value.lower()
        return value

    if cells:
        if cells is True:
            # Transforma todas as c√©lulas do DataFrame
            df = df.map(transform_value)
        else:
            # Transforma apenas as c√©lulas especificadas
            for row, col in cells:
                if row < len(df) and col < len(df.columns):
                    df.iat[row, col] = transform_value(df.iat[row, col])
    elif columns:
        if columns is True:
            # Transforma todas as colunas do DataFrame
            df = df.map(transform_value)
        else:
            # Transforma apenas as colunas especificadas
            for col in columns:
                if col in df.columns:
                    df[col] = df[col].apply(transform_value)
    else:
        # Transforma todo o DataFrame
        df = df.map(transform_value)

    return df


def filter_columns(df: pd.DataFrame, columns: list, allow_partial: bool = True) -> pd.DataFrame:
    """
    Filtra as colunas de um DataFrame com base em uma lista de colunas fornecida.

    Args:
        df (pd.DataFrame): DataFrame a ser filtrado.
        columns (list): Lista de colunas a serem mantidas no DataFrame.
        allow_partial (bool): Se True, mant√©m apenas as colunas existentes no DataFrame.
                             Se False, gera um erro se alguma coluna n√£o existir.

    Returns:
        pd.DataFrame: DataFrame filtrado com as colunas especificadas.

    Raises:
        ValueError: Se `allow_partial` for False e alguma coluna n√£o existir no DataFrame.
    """
    # Verifica as colunas que existem no DataFrame
    existing_columns = [col for col in columns if col in df.columns]

    # Se n√£o permitir parcial e houver colunas faltantes, gera um erro
    if not allow_partial and len(existing_columns) != len(columns):
        missing_columns = [col for col in columns if col not in df.columns]
        raise ValueError(f"As seguintes colunas est√£o ausentes no DataFrame: {missing_columns}")

    # Retorna o DataFrame filtrado com as colunas existentes
    return df[existing_columns]


def rename_columns(df: pd.DataFrame, rename_dict: Union[dict, "Box"]) -> pd.DataFrame:
    """
    Renomeia as colunas de um DataFrame de forma resiliente, lidando com colunas NaN e colunas inexistentes.

    Args:
        df (pd.DataFrame): DataFrame cujas colunas ser√£o renomeadas.
        rename_dict (Union[dict, Box]): Dicion√°rio ou Box (Dynaconf) contendo o mapeamento de renomea√ß√£o.

    Returns:
        pd.DataFrame: DataFrame com as colunas renomeadas.
    """
    # Converte Box para dict, se necess√°rio
    if not isinstance(rename_dict, dict):
        rename_dict = dict(rename_dict)

    # Substitui colunas NaN por strings vazias
    df.columns = df.columns.fillna("")

    # Filtra o rename_dict para incluir apenas colunas que existem no DataFrame
    valid_rename_dict = {col: rename_dict[col] for col in rename_dict if col in df.columns}

    # Renomeia as colunas do DataFrame
    df = df.rename(columns=valid_rename_dict)

    return df


def select_columns(df: pd.DataFrame, target_columns: list) -> pd.DataFrame:
    """
    Seleciona colunas de um DataFrame com base em uma lista de colunas alvo, mantendo a ordem fornecida.

    Args:
        df (pd.DataFrame): DataFrame original.
        target_columns (list): Lista de nomes de colunas desejadas.

    Returns:
        pd.DataFrame: DataFrame com as colunas correspondentes selecionadas.
    """
    # Verifica quais colunas da lista alvo existem no DataFrame
    existing_columns = [col for col in target_columns if col in df.columns]

    # Retorna o DataFrame com as colunas existentes na ordem fornecida
    return df[existing_columns]


def export_to_json(
    data: Union[pd.DataFrame, Dict[str, Union[pd.DataFrame, dict]]],
    file_path: Union[str, Path],
    create_dirs: bool = True,
    orient: str = "records",
    **kwargs,
) -> None:
    """
    Exporta dados para o formato JSON, com suporte para criar diret√≥rios automaticamente.

    Args:
        data (Union[pd.DataFrame, Dict[str, Union[pd.DataFrame, dict]]]): DataFrame, dicion√°rio de DataFrames ou dicion√°rio de dados para exporta√ß√£o.
        file_path (Union[str, Path]): Caminho onde o arquivo JSON ser√° salvo.
        create_dirs (bool): Se True, cria diret√≥rios automaticamente se n√£o existirem. Default √© True.
        orient (str): Orienta√ß√£o do JSON (ex.: "records", "split", "index", etc.). Default √© "records".
        **kwargs: Argumentos adicionais passados para `DataFrame.to_json`.

    Raises:
        ValueError: Se o tipo de dado n√£o for suportado.
        RuntimeError: Se ocorrer um erro durante a exporta√ß√£o.
    """
    def default_serializer(obj):
        """Serializador padr√£o para objetos n√£o serializ√°veis pelo JSON."""
        if hasattr(obj, "__dict__"):
            return obj.__dict__
        return str(obj)

    file_path = Path(file_path)

    if create_dirs:
        file_path.parent.mkdir(parents=True, exist_ok=True)

    try:
        if isinstance(data, pd.DataFrame):
            # Exporta um √∫nico DataFrame para JSON
            data.to_json(file_path, orient=orient, **kwargs)
        elif isinstance(data, dict):
            # Verifica se os valores do dicion√°rio s√£o DataFrames ou outros dicion√°rios
            json_data = {
                key: (df.to_dict(orient=orient) if isinstance(df, pd.DataFrame) else df)
                for key, df in data.items()
            }
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(json_data, f, ensure_ascii=False, indent=4, default=default_serializer)
        else:
            raise ValueError("O tipo de dado fornecido n√£o √© suportado. Use um DataFrame ou um dicion√°rio de DataFrames/dados.")
    except Exception as e:
        raise RuntimeError(f"Erro ao exportar para JSON em {file_path}: {str(e)}")


# Example usage:
if __name__ == "__main__":
    # Reading example
    try:
        df = read_data("sample.csv")
        print("Data read successfully")
    except Exception as e:
        print(f"Error reading data: {e}")

    # Single DataFrame export example
    try:
        df = pd.DataFrame({"A": [1, 2], "B": [3, 4]})
        export_data(df, "output/single_sheet.xlsx", create_dirs=True)
        print("Single sheet exported successfully")
    except Exception as e:
        print(f"Error exporting single sheet: {e}")

    # Multi-sheet Excel export example
    try:
        sheets = {
            "Sheet1": pd.DataFrame({"A": [1, 2], "B": [3, 4]}),
            "Sheet2": pd.DataFrame({"C": [5, 6], "D": [7, 8]}),
        }
        export_data(sheets, "output/multi_sheet.xlsx", create_dirs=True)
        print("Multiple sheets exported successfully")
    except Exception as e:
        print(f"Error exporting multiple sheets: {e}")

    # JSON export example
    try:
        export_to_json(df, "output/data.json", create_dirs=True)
        print("Data exported to JSON successfully")
    except Exception as e:
        print(f"Error exporting to JSON: {e}")
