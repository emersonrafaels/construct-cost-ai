"""
M√≥dulo: budget_reader
------------------------

Respons√°vel por localizar e extrair, de forma robusta e independente de posi√ß√£o,
a tabela principal de itens de or√ßamento no padr√£o utilizado pelas construtoras
(GOT), normalmente presente na aba "LPU" das planilhas recebidas.

Este m√≥dulo foi projetado para suportar arquivos enviados por diferentes fornecedores.

üß© O que este m√≥dulo faz:
-------------------------
1) L√™ a aba "LPU" como planilha bruta, sem assumir header fixo.
2) Localiza dinamicamente o cabe√ßalho da tabela contendo as colunas:

   Filtro | ID | Descri√ß√£o | Un. | Unit√°rio | Coment√°rio | Quantidade | Total

3) Extrai somente a tabela ‚Äî ignorando cabe√ßalhos superiores, metadados e rodap√©s.
4) Normaliza e limpa linhas vazias ou inv√°lidas.
5) Disponibiliza as seguintes fun√ß√µes:

    - ler_planilha_tabela_orcamento(caminho_arquivo, nome_aba="LPU"):
         Fun√ß√£o principal para ingest√£o de um arquivo no padr√£o das construtoras.

   - localizar_tabela(df):
         Detecta a posi√ß√£o (linha, coluna) onde o cabe√ßalho se inicia.

   - extrair_tabela(df, header_row, first_col):
         Extrai todas as linhas subsequentes da tabela at√© as linhas vazias.

üéØ Por que este m√≥dulo √© essencial:
----------------------------------
- As planilhas reais apresentam alta variabilidade de layout.
- A posi√ß√£o da tabela nunca √© garantida: pode estar na linha 10, 20, 40 ou mais.
- O verificador precisa garantir parsing est√°vel antes de aplicar regras de IA.
- Pessoas usu√°rias enviam arquivos com linhas extras, logos, disclaimers etc.
- Minimiza erros de ingest√£o e padroniza o fluxo interno do Verificador Inteligente.

üöÄ Benef√≠cios para o Verificador Inteligente de Or√ßamentos:
-----------------------------------------------------------
- Padroniza leitura ‚Üí menos erros nas etapas determin√≠sticas.
- Permite parsing massivo de or√ßamentos (processamento di√°rio / batch).
- Suporta testes automatizados com grande variedade de layouts reais.
- Fornece uma base estruturada para agentes de IA avaliarem pre√ßos, quantidades,
  desvios e itens fora da LPU.

üìÅ Localiza√ß√£o:
--------------
O m√≥dulo faz parte da arquitetura "utils/", permitindo reutiliza√ß√£o limpa em:

- CLI (rich)
- Orquestra√ß√µes de agentes
- Fluxos Streamlit
- Pipelines via Step Functions / Lambda
- Testes automatizados

"""

__author__ = "Emerson V. Rafael (emervin)"
__copyright__ = "Verificador Inteligente de Or√ßamentos de Obras"
__credits__ = ["Emerson V. Rafael", "Lucas Ken", "Clarissa Simoyama"]
__license__ = "MIT"
__version__ = "1.0.0"
__maintainer__ = "Emerson V. Rafael (emervin), Lucas Ken (kushida), Clarissa Simoyama (simoyam)"
__squad__ = "DataCraft"
__email__ = "emersonssmile@gmail.com"
__status__ = "Production"

import math
import sys
from pathlib import Path
from typing import Union, Optional

import pandas as pd
from pydantic.dataclasses import dataclass

# Adicionar src ao path
base_dir = Path(__file__).parents[4]
sys.path.insert(0, str(Path(base_dir, "src")))

from config.config_logger import logger
from utils.data.data_functions import read_data

# Cabe√ßalho esperado da tabela
EXPECTED_COLUMNS = [
    "Filtro",
    "ID",
    "Descri√ß√£o",
    "Un.",
    "Unit√°rio",
    "Coment√°rio",
    "Quantidade",
    "Total",
]

ALTERNATIVE_COLUMNS = [
    "ID",
    "Un.",
    "Unit√°rio",
    "Quantidade",
]

@dataclass
class FileInput:
    file_path: str
    sheet_name: Optional[str] = "LPU"
    

def locate_table(df, expected_columns=EXPECTED_COLUMNS, 
                 alternative_columns=ALTERNATIVE_COLUMNS):
    """
    Procura no DataFrame a linha/coluna onde come√ßa o cabe√ßalho da tabela:
    Filtro | ID | Descri√ß√£o | Unidade | Pre√ßo Unit√°rio | Coment√°rio | Quantidade | Total
    ou uma alternativa m√≠nima contendo as colunas: ID, Un., Unit√°rio, Quantidade.
    
    Retorna (linha, coluna) do in√≠cio do cabe√ßalho.

    Args:
        df (pd.DataFrame): DataFrame contendo os dados da planilha.
        expected_columns (list): Lista de colunas esperadas no cabe√ßalho da tabela.

    Returns:
        tuple: Uma tupla (linha, coluna) indicando a posi√ß√£o do cabe√ßalho, ou (None, None) se n√£o encontrado.
    """
    # Normaliza os nomes das colunas esperadas para letras min√∫sculas
    normalized_expected = [col.lower() for col in expected_columns]

    # Define um conjunto m√≠nimo de colunas que tamb√©m pode ser aceito como cabe√ßalho
    normalized_alternative = [col.lower() for col in alternative_columns]

    # Calcula o n√∫mero de colunas esperadas
    num_cols = len(expected_columns)

    # Itera sobre todas as linhas do DataFrame
    for row in range(df.shape[0]):
        # Itera sobre todas as colunas poss√≠veis, garantindo espa√ßo suficiente para as colunas esperadas
        for col in range(df.shape[1] - num_cols + 1):
            # Extrai os valores do trecho correspondente √†s colunas esperadas
            values = df.iloc[row, col : col + num_cols].tolist()

            # Normaliza os valores extra√≠dos (remove espa√ßos, converte para min√∫sculas, substitui NaN por vazio)
            normalized = [
                "" if pd.isna(val) or (isinstance(val, float) and math.isnan(val)) else str(val).strip().lower()
                for val in values
            ]

            # Verifica se os valores normalizados correspondem √†s colunas esperadas
            if normalized == normalized_expected or all(col in normalized for col in normalized_alternative):
                return row, col  # Retorna a posi√ß√£o do cabe√ßalho

    # Retorna None, None se o cabe√ßalho n√£o for encontrado
    return None, None


def extract_table(df, header_row, first_col, expected_columns=EXPECTED_COLUMNS):
    """
    A partir da posi√ß√£o do cabe√ßalho, extrai a tabela at√© as linhas vazias.

    Args:
        df (pd.DataFrame): DataFrame contendo os dados da planilha.
        header_row (int): Linha onde o cabe√ßalho da tabela come√ßa.
        first_col (int): Coluna onde o cabe√ßalho da tabela come√ßa.
        expected_columns (list): Lista de colunas esperadas na tabela.

    Returns:
        pd.DataFrame: DataFrame contendo apenas a tabela extra√≠da e processada.
    """
    # Calcula o n√∫mero de colunas esperadas
    num_cols = len(expected_columns)

    # Extrai os dados a partir da linha seguinte ao cabe√ßalho e das colunas esperadas
    data = df.iloc[header_row + 1 :, first_col : first_col + num_cols].copy()

    # Define os nomes das colunas do DataFrame extra√≠do
    data.columns = expected_columns

    # Remove linhas completamente vazias
    data = data.dropna(how="all")

    # Remove linhas onde a coluna "Filter" est√° vazia (geralmente rodap√©s ou espa√ßos)
    data = data[~data["Filter"].isna()]

    # Reseta o √≠ndice do DataFrame para uma sequ√™ncia cont√≠nua
    data = data.reset_index(drop=True)

    # Retorna o DataFrame processado
    return data


def read_budget_table(file_path, sheet_name="LPU"):
    """
    L√™ a planilha (aba LPU) e retorna apenas a tabela de or√ßamento
    no formato Filtro | ID | Descri√ß√£o | ... | Total.
    """
    # L√™ tudo como planilha "crua", sem header
    raw_df = read_data(file_path, 
                       sheet_name=sheet_name, 
                       header=None)

    # Localiza o cabe√ßalho da tabela
    row, col = locate_table(raw_df)
    if row is None:
        raise ValueError("Cabe√ßalho da tabela n√£o encontrado na planilha.")

    # Extrai s√≥ a tabela
    return extract_table(raw_df, row, col)


def orchestrate_budget_reader(*files: FileInput):
    """
    Orquestra a execu√ß√£o do budget_reader.

    Args:
        *files: Lista de inst√¢ncias FileInput contendo o caminho do arquivo e, opcionalmente, o nome da aba.

    Returns:
        pd.DataFrame: DataFrame concatenado de todas as tabelas processadas.
    """
    all_tables = []

    for file_input in files:
        
        logger.info(f"Iniciando processamento do arquivo: {file_input.file_path}, aba: {file_input.sheet_name}")
        
        try:
            table = read_budget_table(file_input.file_path, sheet_name=file_input.sheet_name)
            table["source_file"] = Path(file_input.file_path).name
            table["sheet_name"] = file_input.sheet_name
            all_tables.append(table)
            logger.success(f"Tabela extra√≠da com sucesso do arquivo: {file_input.file_path}, aba: {file_input.sheet_name}")
        except Exception as e:
            logger.error(f"Erro ao processar o arquivo {file_input.file_path}, aba {file_input.sheet_name}: {e}")

    if all_tables:
        final_df = pd.concat(all_tables, ignore_index=True)
        logger.success("Todas as tabelas foram concatenadas com sucesso.")
        logger.info(final_df)
        return final_df

    logger.warning("Nenhuma tabela foi processada com sucesso.")
    return pd.DataFrame()
