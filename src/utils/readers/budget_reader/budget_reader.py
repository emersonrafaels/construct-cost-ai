"""
M√≥dulo: budget_reader
------------------------

Respons√°vel por localizar e extrair, de forma robusta e independente de posi√ß√£o,
a tabela principal de itens de or√ßamento no padr√£o utilizado pelas construtoras
(GOT), normalmente presente na aba "LPU" das planilhas recebidas.

Este m√≥dulo foi projetado para suportar arquivos enviados por diferentes fornecedores.

üß© O que este m√≥dulo faz:
-------------------------
1) L√™ a aba "LPU" como planilha bruta, sem assumir header fixo.
2) Localiza dinamicamente o cabe√ßalho da tabela contendo as colunas:

   Filtro | ID | Descri√ß√£o | Un. | Unit√°rio | Coment√°rio | Quantidade | Total

3) Extrai somente a tabela ‚Äî ignorando cabe√ßalhos superiores, metadados e rodap√©s.
4) Normaliza e limpa linhas vazias ou inv√°lidas.
5) Disponibiliza as seguintes fun√ß√µes:

    - ler_planilha_tabela_orcamento(caminho_arquivo, nome_aba="LPU"):
         Fun√ß√£o principal para ingest√£o de um arquivo no padr√£o das construtoras.

   - localizar_tabela(df):
         Detecta a posi√ß√£o (linha, coluna) onde o cabe√ßalho se inicia.

   - extrair_tabela(df, header_row, first_col):
         Extrai todas as linhas subsequentes da tabela at√© as linhas vazias.

üéØ Por que este m√≥dulo √© essencial:
----------------------------------
- As planilhas reais apresentam alta variabilidade de layout.
- A posi√ß√£o da tabela nunca √© garantida: pode estar na linha 10, 20, 40 ou mais.
- O verificador precisa garantir parsing est√°vel antes de aplicar regras de IA.
- Pessoas usu√°rias enviam arquivos com linhas extras, logos, disclaimers etc.
- Minimiza erros de ingest√£o e padroniza o fluxo interno do Verificador Inteligente.

üöÄ Benef√≠cios para o Verificador Inteligente de Or√ßamentos:
-----------------------------------------------------------
- Padroniza leitura ‚Üí menos erros nas etapas determin√≠sticas.
- Permite parsing massivo de or√ßamentos (processamento di√°rio / batch).
- Suporta testes automatizados com grande variedade de layouts reais.
- Fornece uma base estruturada para agentes de IA avaliarem pre√ßos, quantidades,
  desvios e itens fora da LPU.

üìÅ Localiza√ß√£o:
--------------
O m√≥dulo faz parte da arquitetura "utils/", permitindo reutiliza√ß√£o limpa em:

- CLI (rich)
- Orquestra√ß√µes de agentes
- Fluxos Streamlit
- Pipelines via Step Functions / Lambda
- Testes automatizados

"""

__author__ = "Emerson V. Rafael (emervin)"
__copyright__ = "Verificador Inteligente de Or√ßamentos de Obras"
__credits__ = ["Emerson V. Rafael", "Lucas Ken", "Clarissa Simoyama"]
__license__ = "MIT"
__version__ = "1.0.0"
__maintainer__ = "Emerson V. Rafael (emervin), Lucas Ken (kushida), Clarissa Simoyama (simoyam)"
__squad__ = "DataCraft"
__email__ = "emersonssmile@gmail.com"
__status__ = "Production"

import math
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd
from pydantic.dataclasses import dataclass

# Adicionar src ao path
base_dir = Path(__file__).parents[4]
sys.path.insert(0, str(Path(base_dir, "src")))

from config.config_logger import logger
from utils.data.data_functions import read_data, transform_case, filter_columns

# Constantes centralizadas
DEFAULT_SHEET_NAME = "LPU"  # Nome padr√£o da aba a ser lida
EXPECTED_COLUMNS = [
    "FILTRO",  # Coluna que indica se a linha deve ser filtrada
    "ID",  # Identificador √∫nico do item
    "DESCRI√á√ÉO",  # Descri√ß√£o do item
    "UN.",  # Unidade de medida
    "UNIT√ÅRIO",  # Pre√ßo unit√°rio
    "COMENT√ÅRIO",  # Coment√°rios adicionais
    "QUANTIDADE",  # Quantidade do item
    "TOTAL",  # Valor total do item
]
ALTERNATIVE_COLUMNS = ["FILTRO", 
                       "ID", 
                       "UN.", 
                       "UNIT√ÅRIO", 
                       "QUANTIDADE"]  # Colunas m√≠nimas alternativas
FILTROS = {"FILTRO": ["SIM"]}  # Nome da coluna usada para filtragem

# Metadados padr√£o
DEFAULT_METADATA_KEYS = {
    "C√ìDIGO_UPE": "UPE",  # C√≥digo UPE
    "NUMERO_AGENCIA": "AG√äNCIA|AGENCIA",  # N√∫mero da ag√™ncia
    "NOME_AGENCIA": "NOME DA AG√äNCIA|NOME DA AGENCIA",  # Nome da ag√™ncia
    "TOTAL": "TOTAL",  # Total geral
    "CONTRATO": "CONTRATO",  # N√∫mero do contrato
    "VERSAO": "VERS√ÉO|VERSAO",  # Vers√£o do documento
    "TIPO": "TIPO",  # Tipo do or√ßamento
    "QUANTIDADE_SINERGIAS": "QUANTIDADE SINERGIAS",  # Quantidade de sinergias
    "PROGRAMA_DONO": "DONO",  # Dono do or√ßamento
}

@dataclass
class FileInput:
    """
    Representa um arquivo de entrada com caminho e nome da aba opcional.

    Attributes:
        file_path (str): Caminho do arquivo.
        sheet_name (Optional[str]): Nome da aba a ser lida (padr√£o: "LPU").
    """
    file_path: str  # Caminho completo do arquivo
    sheet_name: Optional[str] = "LPU"  # Nome da aba a ser lida, padr√£o "LPU"

# Fun√ß√µes auxiliares

# Fun√ß√£o para normalizar uma lista de valores, removendo espa√ßos, convertendo para letras min√∫sculas e tratando valores NaN
def normalize_values(values: list) -> list:
    """
    Normaliza uma lista de valores, removendo espa√ßos, convertendo para letras min√∫sculas
    e substituindo valores NaN ou vazios por strings vazias.

    Args:
        values (list): Lista de valores a serem normalizados.

    Returns:
        list: Lista de valores normalizados.
    """
    return [
        "" if pd.isna(val) or (isinstance(val, float) and math.isnan(val)) else str(val).strip().lower()
        for val in values  # Remove espa√ßos, converte para min√∫sculas e trata NaN
    ]

# Fun√ß√£o para pr√©-processar o DataFrame, removendo linhas completamente em branco e resetando o √≠ndice
def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    Realiza o pr√©-processamento inicial dos dados, incluindo a remo√ß√£o de linhas totalmente em branco.

    Args:
        df (pd.DataFrame): DataFrame bruto lido da planilha.

    Returns:
        pd.DataFrame: DataFrame pr√©-processado.
    """
    # Remove linhas vazias e reseta o √≠ndice
    df = df.dropna(how="all").reset_index(drop=True)

    # Converte todas as colunas e celulas em uppercase
    return transform_case(df=df, to_upper=True, columns=True, cells=True)

# Fun√ß√£o para localizar dinamicamente o cabe√ßalho da tabela no DataFrame
def locate_table(
    df: pd.DataFrame,
    expected_columns: list = EXPECTED_COLUMNS,
    alternative_columns: list = ALTERNATIVE_COLUMNS,
) -> Tuple[Optional[int], Optional[int], Optional[list]]:
    """
    Detecta a posi√ß√£o (linha, coluna) onde o cabe√ßalho da tabela come√ßa.

    Args:
        df (pd.DataFrame): DataFrame contendo os dados da planilha.
        expected_columns (list): Lista de colunas esperadas no cabe√ßalho da tabela.
        alternative_columns (list): Lista alternativa m√≠nima de colunas aceitas.

    Returns:
        tuple: Uma tupla (linha, coluna, colunas_encontradas) indicando a posi√ß√£o do cabe√ßalho e as colunas encontradas,
               ou (None, None, None) se n√£o encontrado.
    """
    # Normaliza colunas esperadas para uppercase
    normalized_expected = [col.upper() for col in expected_columns]
    
    # Normaliza colunas alternativas para uppercase
    normalized_alternative = [col.upper() for col in alternative_columns]
    
    # N√∫mero de colunas esperadas
    num_cols = len(normalized_expected)

    # Itera sobre as linhas do DataFrame
    for row in range(df.shape[0]):
        
        # Itera sobre as colunas poss√≠veis
        for col in range(df.shape[1] - num_cols + 1):
            
            # Extrai valores da linha e colunas
            values = df.iloc[row, col : col + num_cols].tolist()
            
            # Normaliza os valores extra√≠dos para uppercase
            normalized = [str(val).upper() if isinstance(val, str) else val for val in values]

            # Verifica se os valores correspondem √†s colunas esperadas
            if normalized == normalized_expected:
                return row, col, expected_columns
            
            # Verifica colunas alternativas
            if all(col in normalized for col in normalized_alternative):
                return row, col, alternative_columns

    # Retorna None se n√£o encontrar o cabe√ßalho
    return None, None, None

# Fun√ß√£o auxiliar para encontrar e atribuir valores de metadados a um dicion√°rio
def find_metadata_value(
    row: pd.Series,
    col_idx: int,
    metadata_key: str,
    metadata: Dict[str, Any],
    df: pd.DataFrame,
    row_idx: int,
) -> None:
    """
    Busca e atribui um valor de metadado ao dicion√°rio, descendo pelas linhas at√© encontrar o valor.

    Args:
        row (pd.Series): Linha do DataFrame.
        col_idx (int): √çndice da coluna atual.
        metadata_key (str): Chave do metadado a ser buscado.
        metadata (dict): Dicion√°rio de metadados.
        df (pd.DataFrame): DataFrame completo para buscar o valor nas linhas subsequentes.
        row_idx (int): √çndice da linha atual no DataFrame.
    """
    # Verifica se o metadado j√° foi atribu√≠do
    if metadata[metadata_key] is None:
        # Itera pelas linhas subsequentes
        for next_row_idx in range(row_idx + 1, len(df)):
            # Obt√©m o valor da c√©lula na linha subsequente
            value = df.iloc[next_row_idx, col_idx]
            if not pd.isna(value):  # Verifica se o valor n√£o √© NaN
                metadata[metadata_key] = str(value).upper()  # Atribui o valor encontrado em uppercase
                break  # Interrompe a busca ao encontrar o valor

# Fun√ß√£o para extrair metadados dinamicamente do DataFrame
def extract_metadata(
    df: pd.DataFrame, metadata_keys: dict = DEFAULT_METADATA_KEYS
) -> Dict[str, Optional[Any]]:
    """
    Extrai metadados da tabela de or√ßamento de forma gen√©rica e din√¢mica.

    Args:
        df (pd.DataFrame): DataFrame contendo os dados da planilha.
        metadata_keys (dict): Dicion√°rio com as chaves de metadados e padr√µes de busca.

    Returns:
        dict: Dicion√°rio contendo os metadados extra√≠dos.
    """
    # Inicializa o dicion√°rio de metadados
    metadata = {key: None for key in metadata_keys}

    # Itera sobre as linhas do DataFrame
    for row_idx, row in df.iterrows():
        for col_idx, cell in enumerate(row):  # Itera sobre as c√©lulas da linha
            if pd.isna(cell):  # Ignora c√©lulas vazias
                continue

            # Normaliza o valor da c√©lula para uppercase
            cell_str = str(cell).strip().upper()

            # Verifica padr√µes de metadados
            for key, pattern in metadata_keys.items():
                # Se o metadado ainda n√£o foi encontrado e o padr√£o corresponde
                if metadata[key] is None and any(p.upper() in cell_str for p in pattern.split("|")):
                    # Busca o valor do metadado
                    find_metadata_value(row, col_idx, key, metadata, df, row_idx)

    return metadata  # Retorna o dicion√°rio de metadados

# Fun√ß√£o para extrair a tabela do DataFrame a partir da linha do cabe√ßalho
def extract_table(
    df: pd.DataFrame,
    header_row: int,
    first_col: int,
    columns_found: list,
    col_filter: str = FILTROS,
) -> pd.DataFrame:
    """
    A partir da posi√ß√£o do cabe√ßalho, extrai a tabela at√© as linhas vazias.

    Args:
        df (pd.DataFrame): DataFrame contendo os dados da planilha.
        header_row (int): Linha onde o cabe√ßalho da tabela come√ßa.
        first_col (int): Coluna onde o cabe√ßalho da tabela come√ßa.
        columns_found (list): Lista de colunas encontradas no cabe√ßalho.
        col_filter (str): Nome da coluna usada para filtrar linhas vazias.

    Returns:
        pd.DataFrame: DataFrame contendo apenas a tabela extra√≠da e processada.
    """
    
    # N√∫mero de colunas encontradas
    num_cols = len(columns_found)
    
    # Define o cabe√ßalho a partir da linha encontrada
    header = df.iloc[header_row, first_col:first_col + num_cols].tolist()
    
    # Extrai os dados abaixo do cabe√ßalho
    data = df.iloc[header_row + 1:, first_col:first_col + num_cols].copy()
    
    # Define o cabe√ßalho no DataFrame
    data.columns = header
    
    # Aplica p√≥s-processamento e filtros
    return post_process_table(data, cols_expected=columns_found, col_filter=col_filter)

# Fun√ß√£o para aplicar filtros e p√≥s-processamento na tabela extra√≠da
def post_process_table(data: pd.DataFrame, 
                       cols_expected: list = [], 
                       col_filter: Dict[str, Any] = FILTROS) -> pd.DataFrame:
    """
    Aplica filtros e p√≥s-processamentos em um DataFrame extra√≠do.

    Args:
        data (pd.DataFrame): DataFrame contendo os dados extra√≠dos da tabela.
        cols_expected (list): Lista de colunas esperadas no cabe√ßalho.
        col_filter (dict): Dicion√°rio onde a chave √© o nome da coluna e o valor pode ser uma string ou uma lista de valores filtr√°veis.

    Returns:
        pd.DataFrame: DataFrame p√≥s-processado com filtros aplicados.
    """
    # Filtra as colunas esperadas, mantendo apenas as colunas encontradas
    if cols_expected:
        data = filter_columns(df=data, 
                              columns=cols_expected, 
                              allow_partial=True)

    # Aplica os filtros definidos no dicion√°rio col_filter
    for col, filter_values in col_filter.items():
        if col in data.columns:
            # Garante que filter_values seja uma lista
            if isinstance(filter_values, str):
                filter_values = [filter_values]
            
            # Filtra linhas onde o valor est√° na lista de valores filtr√°veis
            data = data[data[col].str.lower().isin([val.lower() for val in filter_values])]

    # Padroniza os dados para uppercase
    data = data.applymap(lambda x: str(x).upper() if isinstance(x, str) else x)

    # Reseta o √≠ndice do DataFrame
    return data.reset_index(drop=True)

# Fun√ß√£o para ler a tabela de or√ßamento do arquivo e aba especificados
def read_budget_table(
    file_path: str, sheet_name: str = DEFAULT_SHEET_NAME
) -> Tuple[pd.DataFrame, Dict[str, Optional[Any]]]:
    """
    L√™ a planilha (aba LPU) e retorna apenas a tabela de or√ßamento.

    Args:
        file_path (str): Caminho do arquivo da planilha.
        sheet_name (str): Nome da aba a ser lida.

    Returns:
        tuple: DataFrame contendo a tabela extra√≠da e dicion√°rio com os metadados.
    """
    # L√™ a planilha sem cabe√ßalho
    raw_df = read_data(file_path, sheet_name=sheet_name, header=None)
    
    # Pr√©-processa os dados
    raw_df = preprocess_data(raw_df)
    
    # Localiza o cabe√ßalho da tabela
    row, col, columns_found = locate_table(raw_df)
    
    # Verifica se o cabe√ßalho foi encontrado
    if row is None:
        raise ValueError("Cabe√ßalho da tabela n√£o encontrado na planilha.")
    
    # Extrai os metadados
    metadata = extract_metadata(raw_df)
    
    # Extrai a tabela
    table = extract_table(raw_df, row, col, columns_found)
    
    # Retorna a tabela e os metadados
    return table, metadata

# Fun√ß√£o para orquestrar o processamento de m√∫ltiplos arquivos de or√ßamento
def orchestrate_budget_reader(*files: List[FileInput]) -> pd.DataFrame:
    """
    Orquestra a execu√ß√£o do budget_reader.

    Args:
        *files (List[FileInput]): Lista de inst√¢ncias FileInput contendo o caminho do arquivo e, opcionalmente, o nome da aba.

    Returns:
        pd.DataFrame: DataFrame concatenado de todas as tabelas processadas.
    """
    all_tables = []  # Lista para armazenar todas as tabelas processadas

    # Itera sobre os arquivos de entrada
    for file_input in files:
        # Loga o in√≠cio do processamento do arquivo
        logger.info(f"Iniciando processamento do arquivo: {file_input.file_path}, aba: {file_input.sheet_name}")
        try:
            # L√™ a tabela
            table, metadata = read_budget_table(file_input.file_path, sheet_name=file_input.sheet_name)
            # Adiciona o nome do arquivo como coluna
            table["source_file"] = Path(file_input.file_path).name
            # Adiciona o nome da aba como coluna
            table["sheet_name"] = file_input.sheet_name
            # Adiciona a tabela √† lista
            all_tables.append(table)
            # Loga o sucesso na extra√ß√£o da tabela
            logger.success(f"Tabela extra√≠da com sucesso do arquivo: {file_input.file_path}, aba: {file_input.sheet_name}")
        except Exception as e:
            # Loga o erro ao processar o arquivo
            logger.error(f"Erro ao processar o arquivo {file_input.file_path}, aba {file_input.sheet_name}: {e}")

    # Verifica se h√° tabelas processadas
    if all_tables:
        # Concatena todas as tabelas
        final_df = pd.concat(all_tables, ignore_index=True)
        # Loga o sucesso na concatena√ß√£o
        logger.success("Todas as tabelas foram concatenadas com sucesso.")
        # Loga o DataFrame final
        logger.info(final_df)
        return final_df

    # Loga o aviso de que nenhuma tabela foi processada
    logger.warning("Nenhuma tabela foi processada com sucesso.")
    # Retorna um DataFrame vazio se nenhuma tabela foi processada
    return pd.DataFrame()
